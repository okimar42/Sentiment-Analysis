from typing import Any, Optional, Tuple, Union, ContextManager, List, Dict
from types import TracebackType

# Basic tensor type
class Tensor:
    def __init__(self, data: Any = ..., *args: Any, **kwargs: Any) -> None: ...
    def __getitem__(self, key: Any) -> "Tensor": ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def __sub__(self, other: Any) -> "Tensor": ...
    def __add__(self, other: Any) -> "Tensor": ...
    def __mul__(self, other: Any) -> "Tensor": ...
    def __truediv__(self, other: Any) -> "Tensor": ...
    def softmax(self, dim: int) -> "Tensor": ...
    def to(self, device: Any = ..., dtype: Any = ..., **kwargs: Any) -> "Tensor": ...
    def cuda(self, device: Optional[Union[int, str]] = ...) -> "Tensor": ...
    def cpu(self) -> "Tensor": ...
    def detach(self) -> "Tensor": ...
    def numpy(self) -> Any: ...
    def size(self, dim: Optional[int] = ...) -> Union[int, Any]: ...
    def shape(self) -> Any: ...
    def device(self) -> Any: ...
    def dtype(self) -> "dtype": ...

class dtype:
    """Torch data type"""
    pass

class device:
    """Torch device"""
    def __init__(self, device: Union[str, int]) -> None: ...

# Data types
float16: dtype
float32: dtype
float64: dtype
int8: dtype
int16: dtype
int32: dtype
int64: dtype
bool_dtype: dtype  # Torch boolean dtype (renamed to avoid conflict with Python bool)
uint8: dtype

# Tensor creation functions
def tensor(data: Any, **kwargs: Any) -> Tensor: ...
def zeros(*size: int, **kwargs: Any) -> Tensor: ...
def ones(*size: int, **kwargs: Any) -> Tensor: ...
def empty(*size: int, **kwargs: Any) -> Tensor: ...
def randn(*size: int, **kwargs: Any) -> Tensor: ...
def load(f: Any, map_location: Any = ..., **kwargs: Any) -> Any: ...
def save(obj: Any, f: Any, **kwargs: Any) -> None: ...

# Neural network functions  
def softmax(input: Tensor, dim: int) -> Tensor: ...

# Context managers
class _NoGradGuard:
    def __enter__(self) -> None: ...
    def __exit__(
        self, 
        exc_type: Optional[type], 
        exc_val: Optional[BaseException], 
        exc_tb: Optional[TracebackType]
    ) -> None: ...

def no_grad() -> _NoGradGuard: ...

class _CudaModule:
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def empty_cache() -> None: ...
    @staticmethod
    def mem_get_info(device: Optional[Union[int, str]] = ...) -> Tuple[int, int]: ...
    @staticmethod
    def device_count() -> int: ...
    @staticmethod
    def get_device_name(device: Optional[Union[int, str]] = ...) -> str: ...
    @staticmethod
    def current_device() -> int: ...
    @staticmethod
    def set_device(device: Union[int, str]) -> None: ...

cuda: _CudaModule

# Garbage collection
class _GCModule:
    @staticmethod
    def collect() -> None: ...

gc: _GCModule

# Module for neural networks
class nn:
    class Module:
        def __init__(self) -> None: ...
        def __call__(self, *args: Any, **kwargs: Any) -> Any: ...
        def forward(self, *args: Any, **kwargs: Any) -> Any: ...
        def eval(self) -> "nn.Module": ...
        def train(self, mode: bool = True) -> "nn.Module": ...
        def parameters(self) -> Any: ...
        def named_parameters(self) -> Any: ...
        def cuda(self, device: Optional[Union[int, str]] = ...) -> "nn.Module": ...
        def cpu(self) -> "nn.Module": ...
        def to(self, device: Any = ..., dtype: Any = ..., **kwargs: Any) -> "nn.Module": ...

def __getattr__(name: str) -> Any: ...